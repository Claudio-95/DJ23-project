date	title	subtitle	claps	responses	author_url	story_url	reading_time (mins)	number_sections	section_titles	number_paragraphs	paragraphs
12/31/2022	SQL Concepts that Beginners Have Trouble With	How do I Write SQL Queries from Scratch When There Are…	180	5	https://towardsdatascience.com/@julia-kho	https://towardsdatascience.com/sql-concepts-that-beginners-have-trouble-with-44ad41243806?source=collection_archive---------0-----------------------	7	6	['SQL Concepts that Beginners Have Trouble With', 'How do I know whether to use a left join, right join, or inner join?', 'Does it matter which table is on the left and which table is on the right when I do a join?', 'Does it matter which column I join on?', 'In a real job environment, how do I know how my SQL query should look like?', 'What do I do if I’m struggling with SQL?']	49	['Member-only story', 'Julia Kho', 'Follow', 'Towards Data Science', '--', '5', 'Listen', 'Share', 'As a mentor for an online analytics bootcamp, I‘ve had many mentees ask me “how do I come up with SQL queries in the real job world when there are no hints provided like in my coursework?”', 'Many websites and courses will teach you how to use SQL, but they don’t teach you how to think through creating SQL queries from scratch. That is, how do you know what the SQL query should look like in real life when there is no handholding, no hints provided by the course.', 'This article is not meant to teach you SQL, but to provide clarifications for SQL beginners so that you can get started with writing your own queries.', 'In this article, I will attempt to clear up the confusion with regards to these specific questions that are commonly asked by my mentees.', 'Again, the purpose of this article is not to teach you how to write SQL, so I assume you know what SQL is and the basics of SQL.', 'Let’s look at a quick example to understand what the outputs are for all three types of joins. Table A, on the left, is a table with customer first and last name. Table B, on the right, is a table with order totals.', 'If you’ve forgotten the differences between all the joins, here’s a visual to refresh your memory.', 'If you do an inner join, you’ll get all records that match between the two tables. As you can see, Cust_ID 2, 4, and 5 are in both tables.', 'INNER JOIN', 'If you do a left join, you’ll get all records that match PLUS all records from the table on the left. We get all customers 1–5 from the left table.', 'LEFT JOIN', 'If you do a right join, you’ll get all records that match PLUS all records from the table on the right. There are 3 orders for 3 customers.', 'RIGHT JOIN', 'The right join gives the same output as the inner join. Let’s take a closer look at the the left join and an inner join. What do you notice about the differences between these outputs? Which one should you be using?', 'The output from the left join contains both the customers that had an order as well as customers that did not have an order. The inner join contains only customers that have an order. The decision on which join to use will depend on what you’re being asked and the purpose of this piece of data.', 'Do we only care about customers that have made an order? Do we want to see which customers don’t have orders? Those are the type of questions that you should think about when deciding what join to do.', 'It does not matter whether a table goes on the “left side” or the “right side” of the join. You can get the same output regardless of where you place it. What does matter is what type of join you use after you decide where to place the table.', 'Example 1', 'Let’s say that I put table Customers on the left and table Orders on the right and do a left join.', 'Example 2', 'Now let’s reverse which tables is on the left and right. Put table Orders on the left this time and table Customers on the right. To get the same output as earlier, you want to do a right join because the Customers table is now on the right instead of the left.', 'As you can see, the results in example 1 and example 2 are exactly the same. Thus, you can change the position of which side the table is on, but make sure you’re doing the correct join to get accurate results!', 'You would have to choose the correct column to join on in order to accurately obtain the data. We always join two tables based on the column in which they are related.', 'In the example earlier, we join ON Orders.Cust_ID = Customers.Cust_ID. This one is easy to spot given that they have the exact column name, therefore it is not a secret that they are related to one another.', 'One common example you might often see is something like this. Table A has a column called ID. Table B has a column called Cust_ID. Even though the column name is not exactly the same, we can infer by logic that ID in Table A is related to Cust_ID in Table B.', 'If you’re lucky, you may find documentation showing an entity relationship diagram that will tell you all the primary and foreign keys in the database. That will be your blueprint for which columns to join on.', 'Note that in some cases, you might find that you have to join on multiple columns. So make sure to take a close look at the tables!', 'To answer this question, there are two aspects I’d like to talk about. The first part that is often confusing for my mentees is “how do I know where to get the data?” That is, what tables should be used?', 'Well, you’ll have to take a look at the tables and figure out where your data is located. Is the data located in just one table? Is the data located across several tables? If so, you’ll have the join the tables together to get your output.', 'Here’s an example of what a database might look like. There are 8 different tables here, each containing specific information related to the name of the table.', 'To figure out what tables to use, you’ll examine the tables and determine where the data you want is located. Do you want product information? Do you want employee information?', 'If you’re unsure about where exactly the data is, I recommend querying the table and seeing a sample output to give you a better idea of what’s in the tables.', 'The second part that is often confusing for my mentees is knowing what exactly the SQL statement should look like. You’ve learned the SELECT FROM WHERE ORDER BY syntax, but how do you know what goes into this template? You will know what to write based on what the request is. That is, what data are you being asked to get from the database? You’ll essentially translate the request to your SQL statement.', 'The way I go about writing my queries is first starting with figuring out what the output should look like. That will guide you in determining what your SQL query should look like.', 'Some of the questions I try to answer initially are the following: What columns are needed? What columns need to be calculated and how should they be calculated. What table(s) are those columns located in?', 'These are useful questions because it will help you decide things such as whether you need to JOIN tables, whether you need to do a GROUP BY, etc.', 'In my opinion, I believe you should learn about relational databases before you even learn about SQL because that will help you to understand how databases and joins work. Try taking a relational database course and see if that helps your understanding of the concepts.', 'In addition, you’ll only get better with practice. Try out different queries and see how the output changes. That will really help you understand how to manipulate the SQl query to get what you need.', 'If you’re writing a large query, start small and make sure that each piece is running as you would expect.', 'Hopefully, this article helped clear some things up for you. For hands on SQL practice, check out these 24 SQL exercises with solutions.', 'www.w3resource.com']
12/31/2022	What Makes Us Different from AI?	Opinion: Evolving AI	255	16	https://towardsdatascience.com/@akshaydagar	https://towardsdatascience.com/similarities-differences-between-humans-and-artificial-intelligence-6d2fb3692584?source=collection_archive---------1-----------------------	9	5	['What Makes Us Different from AI?', 'The Ability to Think Creatively', 'Self-awareness', 'Asking the AI', 'The way ahead']	50	['Akshay Dagar', 'Follow', 'Towards Data Science', '--', '15', 'Listen', 'Share', 'The human brain is currently the best Neural Network in the world. Have you ever woken up and felt puzzled about where you were only to realize that you are not in your own home but at someone else’s house where you had stayed the night before? What is the cause of this feeling of disorientation?', 'This is because your brain has learned, over the years, what your room and the world generally look like when you wake up. It tries to predict what you should be seeing based on this learned information, and then gradually adjusts to the new reality when it realizes its mistake. And that is exactly how an Artificial Neural Network (ANN) works. It is fed some training data and the desired outputs for that data and by trial and error, it gradually becomes more accurate in predicting outputs as it learns to closely match the desired outputs of the training data, after which it can predict on previously unseen data with fairly high accuracy.', 'Over the past month, the world, especially the part which spends most of its life trying to make sense of sentences made of strangely ordered English words on an LED screen (aka programmers), has been going crazy over ChatGPT, OpenAI’s new ANN-based chatbot. People fear that it will take away their jobs and YouTube is flooded with tons of videos about how it can write code for complex full-stack applications (like a to-do list) with just a single prompt. Not only programming, ChatGPT is able to write poetry as well:', 'With every stroke of its digital key,', 'Words flow like a river, endlessly.', 'No writer’s block, no need for sleep,', 'ChatGPT works, with data it does keep.', 'As Artificial Intelligence continues to advance, it is likely that it will become increasingly integrated into a wide range of fields and industries such as medicine, where it has been found to be able to diagnose more accurately than doctors, warfare, where it is being used to develop autonomous weapons systems, music, where it is able to generate original compositions based on specified styles or parameter, and even art (all images in this article are generated using an AI called “Stable Diffusion”).', 'So, is Artificial Intelligence going to make humans obsolete? The answer to this question lies in another question: What can our brain do that an Artificial Neural Network can never do? Such abilities will be our future, rest will be automated.', 'The first answer that came into my mind was: “Humans can think creatively whereas Artificial Neural Networks cannot”.', 'Indeed, what is ChatGPT but a model that spews infinite permutations and combinations of small subsets of the knowledge that exists on the Internet (kind of a better version of Google)? ANNs can only combine (or be influenced by) pre-existing knowledge and ideas (that they have been trained on) to generate new ideas. That would mean that future humans only need to be creative thinkers and their ideas will be implemented by their AI “slaves”. I made my peace with that.', 'But then one day I read the book: “Steal Like an Artist” by Austin Kleon. The book discusses how creativ individuals generate ideas by drawing upon and combining previously existing ideas. The author uses numerous examples to illustrate this concept, arguing that all innovative ideas are simply variations of multiple older ideas and that newer ideas are shaped by their predecessors.', 'I started researching where human beings get new ideas from. And while reading dozens of articles about scientific discoveries, I noticed a pattern in history, of so many landmark discoveries being made independently at the same time by multiple people. These include the Telephone, the Steam Engine, Oxygen, Calculus, the Lightbulb, and the Big Bang Theory. An interesting read on this topic: How common is independent discovery? — by Matt Clancy. Here is an excerpt from it:', 'Once a new invention or discovery is “close” to existing knowledge, then multiple people are likely to have the idea at the same time. It also implies scientific and technological advance have some built in redundancy: if Einstein had died in childhood, someone else would have come up with relativity.', 'In my opinion, this shows that the human brain, just like ANNs, combines (or is heavily influenced by) existing ideas to form new ones. Something cannot be created from nothing. The ability to create new ideas is not something that is uniquely human and that Artificial Intelligence can never learn.', 'My next answer was: “Humans are conscious, self-aware beings”.', 'What is self-awareness? It is something that one cannot prove even exists. The best answer to this question that I could find was: it is the ability to think about thinking (to introspect). But the more I thought about it, I realized that AI could also be trained to think about its thought process. It could be trained to project its consciousness outwards and see itself in third-person mode.', 'Is human self-awareness a myth (are we fooling ourselves that we are sentient)? Why have I been staring at my computer, furiously beating my keyboard for the past 2 hours? Maybe we are just pre-programmed “evolutionary”, continuously-learning neural networks that are designed to share their ideas with each other in order to generate newer ideas, about everything, including our own process of thinking.', 'And if there is no such thing as sentience, then free will also doesn’t exist, everything is determined, just like the outputs of an artificial neural network.', 'I am not entirely sure that there is such a thing as self-awareness that is unique to living beings. It probably is just electric signals in our brain, receiving input (ex: this article) and producing output (a mix of these ideas and some others you read somewhere else, unique for everyone). And this neural network is continuously learning (meaning that reading this might have infinitesimally changed the parameters inside the hidden layers of the network called your brain so that this will henceforth influence whatever you do in life in a small way which can affect the world in a big way (the butterfly effect)). Current artificial neural network technology has not achieved the ability for continuous learning, meaning that the capabilities of the human brain are still well beyond the reach of Artificial Neural Networks.', 'An interesting read on Artificial Intelligence consciousness.', 'Unable to find a satisfactory answer myself, I asked ChatGPT what can humans do that Artificial Intelligence can never do.', 'It listed these 5 things:', 'Seems to me like ChatGPT underestimates itself (and other AI).', 'I have already discussed #1 above. #4 and #5 are easily dismissed, Artificial Intelligence might not be as capable at these tasks as humans right now, but with sufficient training, it can be (just like it is not as capable of coming up with new thoughts and ideas as well as humans right now, but it can be in the future, by being built and trained in the right way).', 'Now let’s look at #2: Emotions. Regardless of the prompt I used, I was unable to elicit an emotional response from ChatGPT:', 'Q. You are pathetic and stupid and can never match a human.', 'A. I’m sorry that you feel that way. As an AI, I do not have feelings or emotions, and my primary function is to generate text based on…', 'But what use would ChatGPT have for emotions? What will it do by feeling angry or jealous for example? As far as I can tell, its goal is to be an AI assistant and repository of knowledge.', 'Let’s analyze an emotion, such as “greed”. If you think about it, then this emotion (even though we are told to refrain from it) has historically been, perhaps one of the greatest drivers of evolution (the others being fear and laziness). So it makes sense for humans to have it in them. Similar arguments can be made for other feelings like anger, pleasure, happiness, embarrassment, and even the sense of taste.', 'I don’t think it is right to say that AI can never have emotions like us, we just need to train it in the right way with the right goal in mind, to be an “evolutionary” neural network (more on that later), and it will be able to feel human-like emotions.', 'What about #3 (Ethics): Can AI not be trained with ethical dilemmas and their answers (just like we are trained as kids about good or bad)? Some people say that AI can learn biases and stereotypes from its training data, I ask don’t we also do that?', 'Humans have the ability to ignore and affirm biases based on the situation and our desires. I believe the same thing can be taught to an AI: to affirm biases if they fulfill its desired goals, otherwise, ignore them.', 'After all this analysis I couldn’t find a satisfactory answer to the original question: what can our brain do that an Artificial Neural Network can never do?', 'AI cannot do most of the things mentioned above YET.', 'It cannot generate ideas effectively and it cannot act on them as humans do. Neither can it continuously learn from new data and it would require a significant level of complexity and understanding of human emotions to create an Artificial Intelligence system that can experience emotions in the same way that humans do. A T-800 is still way beyond reach.', 'Its contextual understanding is still not as good as a human’s, hence it is not good at interpreting humor:', 'Q. When is your human zoo coming up?', 'A. As an AI assistant, I do not have the ability to create or organize events. I do not possess a physical body and therefore am not capable of participating in events such as a human zoo…', 'The stories ChatGPT writes mostly follow a definite pattern and contain similar words. Neither does AI have physical abilities as humans do. For now (perhaps for decades to come), it seems like AI will only act as the assistant of humans, making our jobs easier and allowing room for more innovation by automating tasks that we would consider mundane.', 'I believe there are two major steps needed to make Artificial Neural Networks as good as the human brain:', 'It is possible that artificial intelligence could eventually make humans obsolete, but this outcome depends on how and with what goal we choose to train it. If AI does eventually surpass human capabilities, it will likely happen gradually over time.', 'Do you think that humans will eventually find themselves in a similar position to Frankenstein, who created a monster that he couldn’t control?']
12/31/2022	2 Different Replace Functions of Python Pandas	And when to use which	155	0	https://towardsdatascience.com/@sonery	https://towardsdatascience.com/2-different-replace-functions-of-python-pandas-c079408de031?source=collection_archive---------2-----------------------	6	5	['2 Different Replace Functions of Python Pandas', 'DataFrame.replace', 'Series.str.replace', 'Cases where both work', 'Conclusion']	36	['Member-only story', 'Soner Yıldırım', 'Follow', 'Towards Data Science', '--', 'Listen', 'Share', 'Pandas is a highly efficient data analysis and manipulation analysis library for Python. Considering the dominance of Python in data science and the amount of data cleaning, manipulation, and analysis work, Pandas is one of the most widely-used tools in the field of data science.', 'I have been using Pandas for doing my job and creating content. It’s been almost 3 years since the first time I wrote Pandas code and I still keep learning new things.', 'Of course, having an active open-source community and being improved constantly is a significant factor for finding new Pandas tricks to learn.', 'In this article, we will go over a specific piece of Pandas: replace functions. I wrote it as plural because there are two different replace functions in Pandas.', 'We will do several examples to learn how these functions work and what they can be used for.', 'Let’s start with creating a sample DataFrame to work with.', 'This function can be used for replacing values in a column or columns. We, of course, need to specify the value to be replaced and the new value.', 'For instance, we can replace “doc” in the profession column with “doctor”.', 'We can also directly apply the function to the DataFrame. In that case, the column name is specified with a Python dictionary.', 'In the previous two examples, the string “doc” was replaced with “doctor”. The “eng” string in the profession column should be replaced with “engineer”.', 'Thanks to the flexibility of Pandas, we can do both replacements in a single operation. Each replacement is written as a key-value pair in the dictionary.', 'Both “doc” and “eng” have been replaced. There is another way of replacing multiple values in a column, which is using Python lists to indicate values to be replaced and the new ones.', 'In the previous two examples, we replaced multiple values in the same column. We can also replace multiple values in different columns by using nested dictionaries.', 'The following code snippet replaced multiple values in the name and profession columns.', 'The replace function available via the str accessor can be used for replacing a part or subsequence of a string.', 'Accessors in Pandas provide functions specific to a particular data type. The str accessor is the one for string operations.', 'The “str.replace” function can be used for replacing a character in a string.', 'The word “Texas” in line 3 above is a subsequence of a string so we can use the “str.replace” to replace it with “TX”.', 'In order to do multiple replacement, we can chain operations as follows:', 'Unlike the “DataFrame.replace”, the “str.replace” cannot be applied on a DataFrame because DataFrame object does not have a str attribute.', 'The “str.replace” can be used for replacing entire strings but make sure the string to be replaced is not a substring in another value. Let’s do an example to demonstrate this case. Here is our DataFrame:', 'Let’s use “str.replace” to replace “doc” with “doctor” in the profession column.', 'The replacement in line 0 is fine but we have a problem in line 1. The “doc” subsequence in the string “doctor” was also replaced with “doctor” so we end up having a string “docdoctor”, which is definitely not what we want.', 'Let’s say we want to replace the values in the category column with integers. We can use both “DataFrame.replace” and “str.replace” for this task.', 'The output is the same except for the data type. When the “str.replace” is used, the data type remains as string (or object). Thus, we need an extra step of data type conversion to have integers representing categories.', 'We have learned two different replace function of Pandas and how they differ. There are cases where one of them is a better choice so it’s best to know both.', 'It is important to note that both these functions support regular expressions (i.e. regex), which make them even more flexible and capable. If you pass a pattern and want it to be handled as a regular expression, just set the value of the regex parameter as True.', 'You can become a Medium member to unlock full access to my writing, plus the rest of Medium. If you already are, don’t forget to subscribe if you’d like to get an email whenever I publish a new article.', 'Thank you for reading. Please let me know if you have any feedback.']
12/31/2022	The Importance of Cross Validation in Machine Learning	Explaining why Machine Learning needs Cross Validation and how it is done in Python	187	0	https://towardsdatascience.com/@niklas_lang	https://towardsdatascience.com/the-importance-of-cross-validation-in-machine-learning-35b728bbce33?source=collection_archive---------3-----------------------	7	8	['The Importance of Cross Validation in Machine Learning', 'Why do you need Cross Validation?', 'What is Overfitting?', 'What does Cross Validation do?', 'How does Hold-Out Cross Validation work?', 'How does the k-Fold Cross Validation work?', 'What are the advantages and disadvantages of Cross Validation?', 'This is what you should take with you']	34	['Member-only story', 'Niklas Lang', 'Follow', 'Towards Data Science', '--', 'Listen', 'Share', 'The cross validation method is used to test trained machine-learning models and to evaluate their performance independently. For this purpose, the underlying data set is divided into training data and test data. However, the model’s accuracy is then calculated exclusively on the test data set to assess how well the model responds to data that has not yet been seen.', 'To train a general machine learning model, one needs data sets so that the model can learn. The goal is to recognize and learn certain structures within the data. Therefore, the size of the dataset should not be neglected, because too little information may lead to wrong insights.', 'The trained models are then used for real applications. That is, they are supposed to make new predictions with data that the AI has not seen before. For example, a Random Forest is trained to classify production parts as damaged or undamaged based on measurement data. The AI is trained with information about former products that are also uniquely classified as damaged or undamaged. Afterward, however, the fully trained model is to decide for new, unclassified parts from production whether they are flawless.', 'In order to simulate this scenario already in training, a part of the data set is deliberately not used for the actual training of the AI, but instead retained for testing in order to be able to evaluate how the model reacts to new data.', 'The targeted withholding of data that is not used for training also has another concrete reason. The aim is to avoid so-called overfitting. This means that the model has adapted too much to the training data set and thus delivers good results for this part of the data, but not for new, possibly slightly different data.', 'Here is an honestly made-up example: Let’s assume we want to train a model that is supposed to deliver the perfect mattress shape as a result. If this AI is trained on the training dataset for too long, it may end up overweighting characteristics from the training set. This happens because the backpropagation still tries to minimize the error of the loss function.', 'In the example, it could lead to the fact that mainly side sleepers are present in the training set and thus the model learns that the mattress shape should be optimized for side sleepers. This can be prevented by not using part of the data set for actual training, i.e. for adjusting the weights, but only for testing the model once against independent data after each training run.', 'Generally speaking, cross validation (CV) refers to the possibility of estimating the accuracy or quality of the model with new, unseen data already during the training process. This means that already during the learning process it is possible to estimate how the AI will perform in reality.', 'In this process, the data set is divided into two parts, namely training data and test data. The training data is used during model training to learn and adjust the weights of the model. The test data, in turn, is used to independently evaluate the accuracy of the model and validate how good the model already is. Depending on this, a new training step is started or the training is stopped.', 'The steps can be summarized as follows:', 'To divide the data set into two groups, there are different algorithms that are chosen depending on the amount of data. The most famous ones are the Hold-Out and the k-Fold Cross Validation.', 'The Hold-Out method is the simplest method to obtain training data and test data. Many people are not familiar with that name, but most will have used it before. This method simply holds out 80% of the data set as training data and 20% of the data set as test data. The split can be varied depending on the data set.', 'Although this is a very simple and fast method, which is also frequently used, it also has some problems. For one thing, it can happen that the distribution of elements in the training data set and test data set are very different. For example, it could happen that boats are much more common in the training data than in the test data. As a result, the trained model would be very good at being able to detect a boat but would be evaluated on how well it detects houses. This would lead to very poor results.', 'In Scikit-Learn there are already defined functions with which the Hold-Out method can be implemented in Python (example of Scikit-Learn).', 'Another problem with hold-out cross validation is that it should only be used with large data sets. Otherwise, there may not be enough training data left to find statistically relevant correlations.', 'The k-Fold Cross Validation remedies these two disadvantages by allowing data sets from the training data to also appear in test data and vice versa. This means that the method can also be used for smaller data sets and it also prevents an unequal distribution of properties between training and test data.', 'The data set is divided into k blocks of equal size. One of the blocks is chosen randomly and serves as the test data set and the other blocks are the training data. Up to this point, it is very similar to the hold-out method. However, in the second training step, another block is defined as the test data, and the process repeats.', 'The number of blocks k can be chosen arbitrarily and in most cases, a value between 5 and 10 is chosen. A too-large value leads to a less biased model, but the risk of overfitting increases. A too-small k value leads to a more biased model, as it then actually corresponds to the hold-out method.', 'Scikit-Learn also provides ready-made functions to implement k-fold cross validation:', 'Cross-validation is a statistical method used to estimate the performance of a Machine Learning model. It is a crucial step in the process of developing a reliable model. In cross-validation, the data is divided into two parts, the training set, and the testing set. The model is trained on the training set and then tested on the testing set. The results of the testing set are used to estimate the performance of the model on new data.', 'The main advantage of cross-validation is that it provides an estimate of the performance of the model on new data, which is important for assessing the model’s generalizability. It also helps to avoid overfitting, which is a common problem in machine learning. Overfitting occurs when the model is too complex and fits the training data too closely, resulting in poor performance on new data.', 'There are several disadvantages of cross-validation. First, it can be computationally expensive, especially when dealing with large datasets. Second, it may not be suitable for all types of data, such as time-series data, which has a natural ordering that cannot be easily randomized. Third, it assumes that the data is independent and identically distributed, which may not be the case in some real-world scenarios.', 'Despite these limitations, a cross-validation is an important tool for assessing the performance of machine learning models. It is widely used in the development of new models and is a critical step in ensuring the reliability and accuracy of the models.', 'towardsdatascience.com', 'towardsdatascience.com', 'towardsdatascience.com', 'medium.com']
12/31/2022	Why Does the FFT Only Work for Highly Composite Inputs?	And how you need to invoke complex numbers to…	14	0	https://towardsdatascience.com/@rohitpandey576	https://towardsdatascience.com/why-does-the-fft-only-work-for-highly-composite-inputs-7bbaa718351e?source=collection_archive---------4-----------------------	25	8	['Why Does the FFT Only Work for Highly Composite Inputs?', 'I) Motivation: polynomials and convolution', 'II) The algorithm in the book', 'III) Back to polynomials', 'Why powers of 2?', 'An FFT for powers of 3', 'Conclusion', 'References']	107	"['Member-only story', 'Rohit Pandey', 'Follow', 'Towards Data Science', '--', 'Listen', 'Share', 'Now, consider that the most influential algorithm in computer vision is convolutional neural networks. So, an algorithm that performs convolutions as efficiently as possible is important. And in order to develop the most efficient algorithm, you need to invoke complex numbers. Via the fast fourier transform (FFT).', 'The foothold complex numbers have on the field of algorithm development is mostly via the FFT algorithm, which has unreasonable influence to the extent that some call it the most important algorithm of the century ex: [Veretasium_fft]. We’ll spend a little time motivating it and then explore an annoying peculiarity it has, getting an appreciation of its workings in the process.', 'Before we obsess about performing a fourier transform fast, let’s get an appreciation for why we’d even care.', 'Chapter 30 of the book “Introduction to algorithms” by Cormen et.al. [clr_text_book] is titled “Polynomials and the FFT”. To get an appreciation of the FFT from an algorithmic perspective, we need to start with polynomials. A one dimensional polynomial is a function in $x$ of the form:', 'The equation above can lead to lines, parabolas, ellipses, etc. Since the highest power of x is n-1, this would be an n-1 degree polynomial if a_{n-1} != 0. Now, how do we represent a one dimensional polynomial in the memory of a computer? The simplest idea would be to take the coefficients, a_0, a_1, … a_{n-1} and put them in an array since the coefficients completely specify the polynomial. This is called the coefficient representation. In an object oriented programming paradigm, we could write a class whose instances (objects) are defined by the coefficient array. Now that we’ve expressed polynomials in a computer, we want to be able to do arithmetic operations on them. Addition is quite simple. You just add the coefficient vectors together. If the arrays corresponding to one of the polynomials is larger than the other, no problem. Make them the same size by padding zeros to the shorter one. Subtraction is really just addition once you negate all coefficients of the second polynomial. Let’s ignore division because dividing two polynomials leads most of the time to a creature that isn’t even a polynomial. Mutiplication is very interesting (neither trivial nor impossible) and we’ll motivate the FFT with this.', 'Let’s say we want to multiply two polynomials — the first polynomial given by the coefficients: a_0, a_1, a_2 and the second one by: b_0, b_1, b_2. Multiplying them together produces the polynomial:', 'This resulting polynomial has coefficients c_0, c_1, c_2, c_3, c_4 where:', 'The equations above seem messy, but they follow simply from the distributive property of multiplication. This means each term in the first polynomial is multiplied to every term in the second polynomial, which suggests a double for-loop, one loop over the terms of the first polynomial and one over the second one.', 'This is implemented in the Python code below that takes two arrays representing the coefficients of the polynomials to be multiplied as input and produces the coefficient array of the multiplied polynomial as output.', 'The function above is nice and simple, just two loops. If the first polynomial is of degree n (meaning array a has n+1 elements including the constant) and the second of degree m, the number of computations (multiplications, additions, etc.) the algorithm performs would roughly be c_1+c_2. (n. m) for some constants c_1 and c_2 (because of the two loops). Another way of saying this is that the time complexity of the algoritm is O(n m) (since the time taken on any computer by it will be proportional to the number of computations performed).', 'For the special case n=m, this becomes O(n²), meaning the running time scales quadratically with the size of the inputs. To keep things simple, we will assume this special case in this section, generalizing to n and m again in section III.', 'Note that the Python code above simply takes two arrays as inputs and produces a single array as output. There is no explicit context on polynomials in the code itself. And indeed, there is a name for the operation that takes the vectors [a_0, a_1, a_2] and [b_0, b_1, b_2] as inputs and produces the vector [c_0, c_1, c_2, c_3, c_4] as output outside the realm of polynomials. It’s called a ‘convolution’. Apart from being equivalent to polynomial multiplication, it is used in probability theory for determining the distribution of the sum of two random variables and is also a fundamental operation in convolutional neural networks, the current cutting edge model in computer vision.', 'To recap, with the coefficient representation of a polynomial, we could perform addition and subtraction trivially, but multiplication was a bit harder. Is there some other way of expressing a polynomial in computer memory that would make multiplication easy?', 'Indeed, there is. If we took any $n$ inputs to the polynomial, x_0, x_1, …x_{n-1}, and evaluated the polynomial at each of them (getting y_0, y_1, … y_{n-1}), then the pairs:', 'would also completely specify the polynomial (called the point-value representation for obvious reasons). This is because plugging each of those x_i values into the polynomial and equating it to y_i produces one equation in the $n$ coefficients, a_0, a_1, … a_{n-1} (which are unknown here). Since there are n equations and n unkonwns, we can retreive all the coefficients from these equations, hence fully specifying the polynomial.', ""If we’re given two polynomials in this representation (both with degree n and with the same x_i’s), multiplying them becomes trivial. If the first polynomial is y_0 at x_0 and the second polynomial is y_0' at x_0, then what is the value of the polynomial that is their product at x_0? Its simply y_0 y_0'. Similarly, we can get the value of the product at all n x_i’s."", 'But there is a problem with this picture. The polynomial we get from multiplying the two n degree polynomials is of degree 2n (because the x^n terms of both will get multiplied to yield a degree 2n term). So, we need 2n point-values. But if we stored just n for both of the input polynomials, we don’t have enough terms for their product, which requires 2n values.', 'What we need is to be able to get the point-value form efficiently from the coefficient form for any number of points we’d like. Then, we can just generate 2n point-values for both polynomials and multiply the y_i’s of both polynomials point-wise for each i to get the point-value representation of the product polynomial.', 'Let’s say we’re in the coefficient form of a polynomial and want to get the point-value form at the points: x_0, x_1, … x_{n-1}. So, we want to find:', 'So, we have about n² multiplications between the coefficients and the powers of x_i’s to be performed. Meaning, this algorithms run time will scale as O(n²). Once we get this point-value representation for both polynomials, doing the actual polynomial multiplication is just another n multiplications which don’t change the complexity, O(n²).', 'Since doing the convolution directly using the Python algorithm from earlier was also an O(n²) operation, this moving to the point-value representation trick hasn’t done much for us so far.', 'But there is another card we can play. The set of x_i’s can be chosen in a way that the moving to the point-value representation becomes much more efficient. The algorithm that works with the special x_i values that give us a big performance boost is the Fast Fourier Transform (FFT). We’ll delve into this algorithm in the next section. As as a teaser, those special x_i values are complex numbers. After that, we’ll pick polynomial multiplication back up.', 'To recap this section, we described one dimensional polynomials (in only one variable, x), considered storing them on a computer and how multiplying them was equivalent to one-dimensional convolution of their coefficients. The dominant computer vision algorithm for a while has been convolutional neural networks. And the fundamental operation there is thinking of the image as a two dimensional array of pixel values and doing a convolution with another, smaller two dimensional array (called the kernel). This convolution operation on the image is useful for purposes besides machine learning like detecting edges, blurring, etc. This is covered very well in the video by 3 Blue 1 Brown [3b1b_convolution].', 'The image can actually be thought of as a polynomial in two variables (x and y) and so can the kernel. The convolution operation is then simply multiplying the polynomial corresponding to the image with that corresponding to the kernel. But that is a topic for another day.', 'In the previous section, we described how our goal was to convert a polynomial represented as an array of coefficients to point-value representation efficiently. The straightforward method involved simply evaluating the polynomial at all the points, and the run time of this scaled as O(n²) with the number of coefficients. This was for a general set of x_i’s however and we promised there would be some set of x_i’s for which it would become more efficient. And those special values are the nth roots of unity. The solutions to the equation:', 'The algorithm in chapter 30 of the [clr_text_book] does this, but with the caveat that the input array *must* be of a size that is a power of 2. This is a bit of a bummer because most inputs you’d encounter in the real world you’d expect to *not* be powers of 2.', 'They say that algorithms that can handle non powers of 2 sized arrays exist, but are beyond the scope of the book. Then, in the exercises, they ask about a corresponding algorithm that would work on powers of 3 instead (but don’t ask for an implementation). And this gives a hint as to how to get to a more general algorithm. For the time being, we’ll assume that $n$ is a power of 2 and describe the algorithm in the book with some pictures in this section (this section should be considered a supplement to the book itself). Then, we will describe what prevents us from passing non-power of 2 inputs to the algorithm. Finally, we’ll generalize to an algorithm that works on inputs that are sized as powers of 3 and describe how a bunch of such algorithms can be combined into a more general algorithm that can work on any input that is a highly composite number. For most applications of FFT, we have some freedom in choosing the size of the input. And since we can always find a highly composite number very close to any number, this suffices.', 'Note: If you’re already familiar with the n-th roots of units, you can skip ahead to the sub-section on the halving lemma.', 'The number i', 'There is no real number such that multiplying with itself will result in -1. At first, people were content to say that the concept of square roots is simply not applicable to -1 and other negative numbers. But even while trying to find even the *real* roots of general cubic equations, the square root of -1 kept popping up (it turns out, it would cancel out eventually to produce a real number). And people eventually said “if the square root of -1 doesn’t exist on the real number line, doesn’t mean it doesn’t exist anywhere”. They created a new number, i explicitly defined to satisfy the property that multiplying with itself would lead to -1. And since it was something that existed outside the domain of “real” numbers, it was called an “imaginary” number. The truth is that all numbers (real or complex) are imaginary constructs we invented to understand the world.', 'The complex plane', 'Once you have the new number i, you can multiply it with 2 and get a new number, 2i. In fact, for every real number, we can multiply it with i and get a whole new number line made of complex numberes. We can put the original number line on the x-axis of a 2-d plane and the new complex numbers on the y-axis. This special kind of 2-d plane is called the “complex plane”. Note that multiplying 0 with i leads to 0 just like with any other number. So, 0 is where the two lines intersect. A point that has coordinates (x,y) on this 2-d plane can be written as: x+iy as shown.', 'The unit circle', 'While the complex plane is fascinating in its entirity, we’ll focus only on the points that lie on the unit circle, centered at 0. The distance to each point from the origin (also the center of the circle) is 1. Any point on the circle (for example the red point in the figure below) is completely specified by the angle, θ it makes with the x-axis. It can be seen that the green and red points are a part of a right angled triangle with hypotenuse $1$ (the radius of the circle). So, the x-coordinate of the red point is the base of that right triangle, cos(θ) and the y-coordinate is the perpendicular of the right triangle which is sin(θ). So, the point itself is represented by the complex number: cos(θ)+i sin(θ). And by Euler’s famous identity, this is nothing but e^{iθ}.', 'So we’ve established that any point on the circle is specified by e^{iθ}. Now, what happens when we raise this number to the power n?', 'We get a new point on the same circle described by the angle, nθ.', 'The roots of unity', 'Now, let’s go back to the problem of finding the n-th roots of unity. The values that satisfy the equation:', 'Let’s start with the right hand side. It’s just a 1. But we want to speak only in terms of points on the unit circle (of the form e^{iθ})? What values of θ lead to 1? Of course, θ=0 satisfies this requirement since any number raise to the power of 0 is 1 and i multiplied by 0 is 0. So, x=e^{i 0} = e⁰ = 1 is always a solution. But there are more.', 'Now, imagine starting at θ=0 and gradually increasing θ. As we do this, we move around the circle.', 'What happens at θ = 2𝜋? We complete one full revolution around it and find ourselves back at 1. So, e^{i 2𝜋} is also equal to 1. As we keep increasing θ beyond this, we take another loop and at θ = 2𝜋+2𝜋=4𝜋, we end up back at 1. The pattern here is that all integer multiples of 2𝜋 end up at 1. So,', 'the equation we wanted to solve becomes:', 'Taking the n-th root of both sides:', ""There now seem to be a (countably) infinite number of solutions for our original equation. So, we went from just one solution (x=1) to infinite now. But if you delve into them a little more, you’ll find there aren’t really infinite distinct solutions. You get n distinct solutions and then they start repeating. The first solution is x=1 corresponding to k=0. The second is ω_n=e^{2i𝜋/n} corresponding to k=1. This is called the “principal root of unity” and all others are integer powers of this one. The third is e^{4𝜋 i/n} = ω_n² for k=2 and so on. Once we get to k=n, we get e^{2𝜋 n/n} = e^{2 𝜋} = 1 (same as when k=0). And with k=n+1, it's the same number as for k=1. So, only values of k ranging from 0 to n-1 produce distinct roots that we didn’t see before."", 'Let’s consider the case of n=3. In other words, we’re now looking to find the cube roots of unity, the numbers that satisfy:', 'Plugging into:', ', we get the following 3 distinct values for k=0,1,2 and then, they just start repeating.', 'The halving lemma', 'Now, we get to the most important concept that is central to how we’re able to obtain the speedup in the fast fourier transform and also to the question around why the FFT only works on highly composite inputs.', 'If you take the n-th roots of unity when $n$ happens to be even and square all of them, you get the n/2-th roots of unity. But from the previous section, we know there are exactly n distinct n-th roots and n/2 distinct n/2-th roots of unity. So when we square the n-th roots, only half of them end up being distinct numbers and the other half just wrap around and repeat the existing ones. And it is the introduction of this redundancy that helps us save some computation and speed things up.', 'This is demonstrated below for the case of n=8. Squaring each of the 8-th roots of unity brings us to four distinct points left standing, the fourth roots of unity.', 'Now, let’s see how the halving lemma is actually utilized to create an efficient algorithm. Remember, the problem statememnt of the FFT is really simple. Given n coefficients of a polynomial (stored in an array), evaluate the polynomial at each of the n-th roots of unity.', 'Thinking first of the naive way to do this that takes O(n²) computations, we can visualize these as a table with the n coefficients (the inputs) placed along the columns and the n roots of unity placed along the rows of the table. In the i,j cell of the table, we raise the corresponding ω of that row to the power j and multiply with a_i, the corresponding coefficient of the column the cell is in. Then, we sum across all rows to get one element per column (see the green parts of the figure below). These n elements are packaged into the output vector, y. The whole process is visualized in the figure below.', 'To improve on the O(n²) computations, we use a trick that leans on the halving lemma from the previous section and helps split the problem into smaller sub-problems. This is the divide and conquer strategy that shows up in many CS algorithms like merge sort, binary search in a sorted array, etc.', 'Since the lemma involved squaring all the n roots of unity, we try to express the equations of interest in terms of only the squares of the original input. All the evaluations we’d like to do are polynomials at each of the $n$ roots of unity of the form:', 'Can we modify this equation such that it only involves polynomials in x²? Let’s pick the low hanging fruit first. The coefficients that already have x² and its powers, which are just the evenly indexed coefficients. Separating those and the remaining (which are the odd indexed coefficients) out, we can write the original polynomial:', 'Now, with the motivation that we’re looking to split the original polynomial into smaller polynomials so we can divide and conquer,', 'So, we’ve succeeded in splitting the original polynomial into two polynomials. And both those polynomials involve only powers of x². Since the first polynomial has even indexed terms from the original polynomial, we can call it A_0(x) (indices such that remainder when divided by 2 is 0) and similarly the second one can be called A_1(x). This gives us:', 'Where:', 'So, the original problem of evaluating A(x) at the n roots of unity, ω_n⁰, ω_n¹, … ω_n^{n-1} reduces to evaluating the polynomials A_0(x) and A_1(x) at the squares of the n roots of unity. Both of the new polynomials are smaller and have n/2 coefficients.', 'And now, the halving lemma from the previous section comes in. When we square each of the n roots of unity, the first n/2 happen to be the distinct n/2-roots of unity. The remaining n/2 of them just loop around and repeat as shown in figure-5. So, we don’t have to evaluate the polynomials for them again. Hence, the number of points where we have to evaluate the polynomial also gets cut in half too. And since the squares of the n roots are the n/2-roots, the two sub-problems involve evaluating polynomials with n/2 coefficients at the n/2 roots of unity and are hence also discrete Fourier transform problems, just with different (smaller) coefficient arrays. The divide step is now complete. But now that we know how to divide, we don’t stop at just one division. Instead, we keep going until we hit the base case, which is an array of length one. At that point, the Fourier transform is the element in the array itself. The process of dividing the problem is core to the speed up the FFT brings. This is visualized in the figure below to further strengthen the intuition. We set up for the n × n multiplications like in figure-6. But then, we split the n × n matrix into two smaller matrices, each of size n × n/2. The red parts of the matrices are just repetitions of the blue parts, so they can be discarded. This makes the two smaller matrices n/2 × n/2.', 'For getting the blue part of the original matrix, the output values from 0 -> n/2–1, we can just use the blue parts of the two split matrices via equation (3). For getting the yellow part of the original matrix, the output values from n/2 -> n-1, we can use the equations below:', 'Now, by definition,', 'Plugging these into equation (4),', 'With this context on how the divide and conquer works, we can introduce the Python code (note: the initial skeleton of this was written by ChatGPT-3, the remainder translated from the pseudo code in chapter 30 of the [clr_text_book]).', 'Line 10 is the base-case of the recursion, an array of size 1. Here, the discrete fourier transform is simply the array itself.', 'Lines 20 and 23 leverage the observation that we can split the FFT into more FFT’s that are half the size.', 'The for loop in line 29 then combines the two smaller FFT’s and produces the larger FFT per equations (3) (line 30) and (4) (line 31).', 'To appreciate the speedup this approach is bringing us over the simpler naive implementation, let’s consider performing the discrete fourier transform of an 8 element array. To count all the operations that need to happen without the fancy divide and conquer, let’s bring back the matrix from figure 6.', 'There are a total of 64 cells in the matrix above and all of them need at least one multiplication (multiplying the coefficient with the complex number). So, that’s at least 64 multiplications (the complex roots need to be calculated as well, but we’ll ignore that as any approach would require it). To add them up and get the output y vector, we’ll need to sum each row. Summing 8 numbers requires 7 additions, so that makes for an additional 7 × 8 = 56 additions. So, the naive approach has 64 coefficient-complex number multiplications and 56 additions. Let’s see how the divide and conquer fares.', 'The way the algorithm in the “fft_2” routine pans out is shown in figure-7 below. We start out with the 8 element array and want to get the DFT. What we reference as y_0 and y_1 in the code, we call y_l and y_r in the diagram below for left and right arrays (since that’s how they’re arranged in the recursion tree). When y_l is split into two smaller arrays, they get the names y_{ll} and y_{lr} etc. We see there are just three recursion levels where arithmetic operations are happening. This isn’t surprising since log_2(8)=3. On the left of the diagram, we see that 8 multiplications and 4 additions happen at each layer in the recursion tree. This makes for a total of 24 additions and 12 multiplications. Contrast this with the 64 multiplications and 56 additions for the naive approach.', 'In general, the problem size at each iteration gets split in two sub-problems half the size of the original one. And we then require one for-loop (or O(n) computations) to combine the outputs from the two sub-problems and solve the original problem of interest. Let’s say that total operations required for an input array of size n is T(n). Every time we split into two sub-problems, we need to do 2T(n/2) work to solve each of them and then O(n) work to combine them, meaning c.n+d for some constants, c and d. This can be expressed:', 'Continuing with this pattern,', 'We stop when n/(2^k)=1 and therefore, n=2^k. This means:', 'At that stage, we get $T(1)$ which is zero additions or multiplications. Plugging into the equation above we get:', 'Which is O(n.log(n)) computations.', 'First, let’s tie up the lose end from section I and describe how this new divide and conquer algorithm is going to help perform polynomial multiplication much faster than the double for-loop. Let’s say we’re given two polynomials expressed as coefficient arrays. And the sizes of those arrays are $n$ and $m$. The two polynomial will look something like:', 'The degree (highest power of x) of the first polynomial is n-1 and that of the second polynomial is n-2. So, the product polynomial, C(x) will have degree n+m-2 and look something like:', 'There are a total of n+m-1 coefficients.', 'To do the multiplication, we want to first convert the two input polynomials to point-value form. Once there, as long as the points at which the two are evaluated are the same, we can multiply point-wise to get the values of $C(x)$. To specify the $n+m-1$ coefficients of $C(x)$, we’ll need $n+m-1$ points to evaluate them at. So, we want $A(x)$ and $B(x)$ to also be calculated at $n+m-1$ points. But, they have just $n$ and $m$ coefficients respectively and applying the Fourier transforms to them will evaluate them at the $n$ and $m$-th roots of unity respectively. So, we make them both have $n+m-1$ coefficients instead. The way to do this is to pretend they are both polynomials of degree $n+m-1$. By just tagging on zero coefficients at the end. So, the two polynomials become:', 'Just pass these larger, zero-padded arrays to the FFT routine and it’ll calculate the values of the two polynomials at each of the n+m-1-th roots of unity. These steps will take O((n+m).log(n+m)) time.', 'Then, multiply these two sets of n+m-1 values point-wise, which takes O(n+m) time. And now that we have the point-values of C(x) at those n+m-1 roots of unity, we can go back to the coefficient representation of that polynomial. This last step is called the inverse Fourier transform (since it does the opposite thing a Fourier transform does). And per theorem 30.7 of the [clr_text_book], this can also be done in O((n+m)log(n+m)) time. In fact, the FFT routine needs to be modified only slightly to convert it into an inverse FFT routine.', 'The overall process takes O((n+m).log(n+m)) time, which is better than the O(nm) of the naive method of doing this.', 'These two approaches are shown pictorially in figure 30.1 of the [clr_text_book] (they consider the case n=m). I’ve shamelessly pasted that figure here.', 'But aren’t we forgetting that the efficient implementation we have only works on powers of 2? What if n+m-1 is not a power of 2? One solution would be to find the next higher number that is a power of 2. If we get lucky, that could be quite close. But what if the next power of 2 is very far away? And why are we stuck with this restriction anyway?', 'At first glance, it would seem like the whole divide and conquer paradigm is the reason we’re stuck with powers of 2. That is until you realize that the [clt_text_book] is full of divide and conquer algorithms. Examples are binary search, sorting algorithms like merge sort and quick sort among many otheres. And none of them have the restriction of only working on inputs that are powers of 2. Even if you implement them assuming the input size is going to be a power of 2 and then give them arrays of sizes that are not powers of 2, they just work for them too. In fact, the FFT is the only algorithm in that book that doesn’t generalize to non-powers of 2. What happens if you just give the fft_2 routine an array that isn’t correctly sized? Nothing breaks, no explosion happens and no error is thrown. But, sadly, the values produced are incorrect. They won’t match up with what you would get from the naive method.', 'The reason for the limitation is the use of the halving lemma from section II. If you take the n-th roots of unity when n is even and square them, you will get the n/2 roots of unity and the last n/2 values will nicely fold into the first n/2. But if n isn’t even, this whole thing breaks down. First, n/2 isn’t even an integer. And squaring the complex numbers doesn’t even result in the [n/2] roots of unity or anything. Which is why assuming that when n isn’t even will just lead to a wrong answer. And because we want to be able to do the split all the way to the base-case, n must be a power of 2 for the fft_2 routine to work.', 'But, what if n isn’t quite a power of 2, but a near miss. Say we have: n=3*2¹⁰. Of course, we could just try to evaluate the FFT at n=4*2¹⁰, but even this would be unncessary if we modify the fft_2 routine very slightly and having a kind of “eject button” when the power of 2 assumption is broken. As we split the 3*2¹⁰-sized array two ways recursively, we’ll keep getting even array sizes until we reach 6. Then, when we split that further into two calls on arrays of size 3, we just default to the naive approach of just evaluating the polynomials. All we need is an “if” condition at the very top checking if n is a power of 2 and falling back to the naive routine if so.', 'But what if we had an array whose input size was n = 3⁷*2²? Once we ran out of two way splits (which’ll happen very quickly), we’d end up with two arrays each sized 3⁷ which are quite large. At that point, you’ll be wishing there was a routine that worked on powers of 3 inputs instead of powers of 2. And in-fact, there is. Just as we can choose the number of “divides” for other divide and conquer algorithms, we can do the same for the FFT. In the next section, we’ll cover a version of the FFT, fft_3 which works on power of 3 sized inputs. Now, we can have a master routine that passes the array to the fft_2 as long as the input is even, to fft_3 if its a multiple of 3 and to the naive implementation as the final fall-back. But then we can have versions for the first m prime numbers, fft_2, fft_3, fft_5, fft_7, fft_11 and so on. Now, as long as we get an input array size that is highly composite, we’re still going to get all the benefit of the FFT. And it is easy to find a highly composite number that is only slightly larger than any n, which suffices for our polynomial multiplication and most other use-cases.', 'Finally, let’s cover an FFT algorithm that works on powers of 3 instead of 2. The approach can obviously be extended to any number (but we’d want to use it for primes). As discussed in the previous section, the reason the fft_2 routine gave incorrect answers for input sizes not powers of 2 was the halving lemma. If we want it to work for powers of 3, we need a corresponding “trisecting lemma”.', 'If $n$ is a power of 3 and you take the n-th roots of unity and cube them all, you get the n/3-th roots of unity. The other 2n/3 roots just wrap around and overlap with the n/3 roots.', 'This will give us the ability to split the FFT into three sub-problems. But, we’d also like to know how to combine them into the FFT array we want. To generalize equation (4) to the case of a three-way split, this is how the Math works out:', 'Where in the last step we used the fact that:', 'With all of these facts, we have the fft_3 algorithm:', 'The FFT is not just an algorithm confined to signal processing, where many learn about it. It has wide ranging applications even extending into unexpected fields like differential equations and deep learning. It is an algorithm that combines the mathematical beauty of complex numbers with cold, efficient algorithmic thinking. Unlike other divide and conquer algorithms, it has a strange quirk that means it works only when divided into exact parts. And this is why its efficiency gain only holds for highly composite inputs. But this isn’t an issue for most practical applications since we have some freedom in choosing the input size. And even for large prime numbers, there are ways to still leverage the efficiency it brings.', 'If you liked the story, become a referred member :)', 'https://medium.com/@rohitpandey576/membership', '[Veretasium_fft]: Veretasium’s video on the FFT algorithm: https://www.youtube.com/watch?v=nmgFG7PUHfo', '[clr_text_book]: Introduction to Algorithms by Cormen et.al. Third edition.', '[3b1b_convolution]: Video by 3 Blue 1 Brown “But what is a convolution” https://www.youtube.com/watch?v=KuXjwB4LzSA']"
12/31/2022	How to Visualize Monthly Expenses in a Comprehensive Way: Develop a Sankey Diagram in R	-	88	0	https://towardsdatascience.com/@chengzhizhao	https://towardsdatascience.com/how-to-visualize-monthly-expenses-in-a-comprehensive-way-develop-a-sankey-diagram-in-r-789df4902cea?source=collection_archive---------5-----------------------	5	7	['How to Visualize Monthly Expenses in a Comprehensive Way: Develop a Sankey Diagram in R', 'What Is Sankey Diagram', 'Prerequisite', 'How To Download Your Monthly Transactions From Mint', 'Create Sankey Diagram In R and ggplot2\u200b', 'Create Sankey Diagram Without Coding\u200b\u200b', 'Final Thoughts']	28	['Member-only story', 'Chengzhi Zhao', 'Follow', 'Towards Data Science', '--', 'Listen', 'Share', 'Visualizing the monthly cash flow isn’t new if you use personal budgeting/finance tools like Mint/Personal Capital/Clarity. All those tools primarily provide three types of charts: pie charts, bar charts, and line charts. However, have you ever wondered if charts are good enough to get better ideas about your monthly income and expense? Are there ways to visualize monthly expenses comprehensively? In this article, I will share with you how to create a Sankey Diagram in R to help better you gain more insights into your financial situation.', 'From Wikipedia: Sankey Diagrams are flow diagrams in which the width of the arrows is proportional to the flow rate. One of the famous Sankey diagrams is Napoleon‘s invasion of Russia. The diagram below clearly shows the time and number of troops left.', 'Back to personal finance monthly cash flow, it is a perfect use case to adopt the Sankey diagram to demonstrate the cash flow and which account the money originates from or goes to.', 'The pie and bar charts in the Mint App don’t show how cash flows but the amount of money spent/earned in one category, which is one of the limitations to deep diving your personal finance by cash flows.', 'Here is one of the nice Sankey diagram on Reddit. Our goal in this article is to recreate a similar one with a data dump from a personal finance app like Mint.', 'Although it requires you to install R to get better-customized results, you can add little code yourselves, and having a deep understanding of R is not required. You can copy & paste the code from this post.', 'You’d need to install R and RStudio to get the best experience in R', 'We will use Mint.com as an example to download the transactions. It is straightforward from the web application, and the mobile APP doesn’t have this option. Once you log on to Mint, go to TRANSACTIONS, scroll to the button, and you should see an option “export all xxx transactions.” A CSV file will be downloaded when you click on that option.', 'The downloaded CSV file has the following fields: Date, Description, Original Description, Amount, Transaction Type, Category, Account Name, Labels, and Note. We will use the Amount, Transaction Type, Category, and Account Name to build the Sankey Diagram.', 'One of the libraries we will use to build the Sankey diagram is ggalluvial. The design and functionality were inspired initially by the alluvial package. One of the great things about ggalluvial is that it builds on top of ggplot2, and you can get the benefit of the grammar of graphics', 'I also wrote an article about Why Is ggplot2 So Good For Data Visualization?', 'towardsdatascience.com', 'Below is the code to extract data from CSV downloaded from mint, perform some basic transformations and use the ggplot to visualize as Sankey diagram.', 'The code above can be broken down into three categories:', 'Now we can visualize the diagram as follows, which clearly shows the cash flow on each category which account they are coming from and how each category is distributed.', 'There is also a website called sankeymatic.com that provides a no-coding option for drawing a nice Sankey diagram. It requires the user to format the input in a certain way, and then you should have the same result as the above Reddit post.', 'The personal finance app provides a quick, easy data visualization, but it limits itself to advanced comprehensive usage like cash flow analysis. I hope this article complements this area of your personal finance analysis. Please let me know what you think about the Sankey diagram and using R to build such an excellent chart by leaving a comment below.', 'I hope this story is helpful to you. This article is part of a series of my engineering & data science stories that currently consist of the following:', 'Chengzhi Zhao', 'You can also subscribe to my new articles or become a referred Medium member who gets unlimited access to all the stories on Medium.', 'In case of questions/comments, do not hesitate to write in the comments of this story or reach me directly through Linkedin or Twitter.']
