date	title	subtitle	claps	responses	author_url	story_url	reading_time (mins)	number_sections	section_titles	number_paragraphs	paragraphs
04/30/2023	The Best Learning Paths for AI and Data Leadership	Making Data Useful	1.4K	8	https://towardsdatascience.com/@kozyrkov	https://towardsdatascience.com/the-best-learning-paths-for-ai-and-data-leadership-fabc3d4f8e36?source=collection_archive---------0-----------------------	7	11	['The Best Learning Paths for AI and Data Leadership', 'Decision Skills', 'Data-Driven Leadership and Careers', 'Making Friends with AI', 'Irreverent Demystifiers', 'Statistical Thinking', 'Making Data Useful', 'How To Hack Yourself', 'Thanks for reading! How about an AI course?', 'Connect with Cassie Kozyrkov', 'Footnote']	27	['Member-only story', 'Cassie Kozyrkov', 'Follow', 'Towards Data Science', '--', '8', 'Listen', 'Share', '(Feeling impatient? Scroll past the text and cat photo to get the learning paths!)', 'First, may I say a huge thank you to all of you for encouraging me to write? I just noticed that here on Medium, my community of followers is 70% the size of Barack Obama’s. Whoa!', 'I’m honored and humbled by all the love this amazing community has given me. I don’t know if it’s despite my being a cheerful weirdo or because of it, but thank you! And thank you for being unresponsive to my job title when I turned it off and on at random several times, with no effect on any of the metrics — it means a lot that you’re here for my thoughts and not my labels. Especially since it has been almost 10 years since I’ve had a career change and who knows what kind of madcap adventure I’ll pick when I eventually decide that a change is a good as a holiday.', 'It was been a great pleasure to share helpful musings with you, but now that I’ve published over 180 blog posts, many of you have told me you’re drowning in all my content and I need to index it better. Turns out it’s very confusing for newcomers to my blog to sort through all the different topics I write about. I hear you! Not everyone is here for all the things. Eventually, I’ll prepare a well-curated site to help you out, but in the meantime, let me take the first step towards a fix by adding standardized supertitles to all my articles. That way you’ll know what category you’re dealing with each time so you can dive right to the ones you care about and skip my musings on random esoterica. In essence, it’ll be as if I have mini-publications for you to chose from.', 'To take a tiny tangent in defense of the wide range of topics is that in my head, they’re all about the same thing: decision intelligence!* No matter how data-oriented the writing, it’s always founded on the principle of improving your real-world actions. Decision intelligence is about giving yourself the skills and tools to turn information (whether it’s your memories of lunch conversations or it’s your foray through a massive database) into better actions (decisions!) at any scale (from tapas bites to petabytes) and in any setting (from picking a college major to building an AI system). I find it perfectly natural to span this full range of topics — necessary, even, for any serious student of decision-making — though I’ll acknowledge that even with 180+ articles, I’m barely scratching the surface of everything worth knowing.', 'But if you’re a bit more narrowly focused, hopefully this new index will add some rhyme and reason to your knowledge feast.', 'This is where you’ll find advice on how to be a better decision-maker, with or without a fancy algorithm. It focuses on the human side of things, like battling your biases, structuring your goals, understanding your irrationality, etc. This is the place for those who seek nuggets of wisdom from disciplines like psychology, economics, neuroscience, managerial science, negotiation, and other classic decision sciences.', 'A category for the data leaders and aspiring leaders among you. This is where I put articles about what’s missing from organizations, what kinds of things you might be doing that cause your data people to quit, whom to hire in what order, how to build a data-driven culture, and so on. I also include data science careers articles from the point of view of the aspiring team member, such as interview questions to ask… which is a handy thing for the manager to read too (it sure helps to know what advice your people are getting about dealing with you).', 'This is where I cover concepts about machine learning and AI in the friendliest way the internet has ever seen, or your (it’s all free!) money back. Some of these articles will be deeper (and snarkier) dives that extend the lessons in my popular Making Friends with Machine Learning (MFML) course on YouTube (the index is here), while others take on the AI zeitgeist or whatever recent misunderstandings I’ve had the pleasure to be subjected to. Immunize yourself here so those same offenses against good sense never cross your own lips.', 'My beloved VC and CEO crowd, run the other way! (Run to any of the categories above, but skip this one.) This one’s for the (eternal) students. Some of you really love it when I pick a random esoteric jargon term and explain the hell out of it for you cheerfully so it feels intuitive. Yes, it’s super nitty-gritty! Yes, most of you don’t care about it! But this stuff is catnip for the, um, perhaps three of you who love to see pompous terminology taken down a notch, shiny new software prodded until it confesses, and formulas explained so a kid (or pointy haired boss) can understand them. So every now and then, I’ll amuse the four of us by showing you how simple we can make complicated things if we understand them deeply. This is also the place where you’ll find out why a topic is where it is in the textbook. Both when it should be where it is and when it most definitely shouldn’t (even if no one told academia yet).', 'I’m a recovering statistician who’s unlikely to ever recover, so there are many things I have to say about statistics. So many! And I’ve said many of them is a 10.5h secret course all about statistical decision-making which I haven’t put online yet (the first half hour is available in bootleg form, but the bulk of it is waiting for a pro camera crew to capture it — until then, the only way to see it is by inviting me to perform it live). Occasionally, I’ll elaborate on some of the things I say in the course and this category is where you’ll find them.', 'Those who have been following me a while will hopefully recognize these three words… “the discipline of making data useful” is my definition of data science. Welcome to the category that spans general data science plus analytics, minus all the topics that already got sucked into the more specialized categories above. If you’re a practicing data scientist, you’ll want to follow this category plus whichever preceding one most floats your boat.', 'If it’s not any of the categories above, then it’s either a summary of advice I gave someone at a Q&A session (often about careers, courage, self improvement, or juggling life) or it’s some kind of skill/insight that made me a little bit better at growing into the version of me that y’all know and love (or love to hate, it’s the internet after all, hi). Examples include public speaking tips, advice for making new years resolutions, and thoughts on math impostor syndrome.', 'Oh, and many of the links in my articles take you to other articles I wrote related to the highlighted word (and other links take you to easter eggs and humor), so my blog is an elaborate network of Choose Your Adventure. Because upgrading ourselves should be fun and involve a touch of capricious serendipity.', 'Enjoy!', '(And don’t forget to let me know which category you’re most excited about, since that’ll help shape the balance of topics I pick.)', 'If you had fun here and you’re looking for an applied AI course designed to be fun for beginners and experts alike, here’s one I made for your amusement:', 'Let’s be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch.', '*Okay, not all of them; I’ll admit that the ones teaching you about public speaking were born out of a capricious impulse.']
04/30/2023	Hyperbolic Deep Reinforcement Learning	RL meets hyperbolic geometry	358	4	https://towardsdatascience.com/@michael-bronstein	https://towardsdatascience.com/hyperbolic-deep-reinforcement-learning-b2de787cf2f7?source=collection_archive---------1-----------------------	17	8	['Hyperbolic Deep Reinforcement Learning', 'Basics of Reinforcement Learning', 'Hyperbolic Geometry', 'Hyperbolicity of RL problems', 'Training agents with hyperbolic latent spaces', 'Experimental results', 'Visualisation', 'Conclusions']	91	['Michael Bronstein', 'Follow', 'Towards Data Science', '--', '4', 'Listen', 'Share', 'Many problems in Reinforcement Learning manifest a hierarchical tree-like nature. Hyperbolic spaces, which can be conceptualised as continuous analogies of trees, are thus suitable candidates to parameterise the agent’s deep model. In this post, we overview the basics of hyperbolic geometry, show empirically that it provides a good inductive bias for many RL problems, and describe a practical regularisation procedure allowing to resolve numerical instabilities in end-to-end optimisation with hyperbolic latent spaces. Our approach shows a near-universal performance improvement across a broad range of common benchmarks both with on-policy and off-policy RL algorithms.', 'This post was co-authored with Edoardo Cetin, Ben Chamberlain, and Jonathan Hunt and is based on the paper E. Cetin et al., Hyperbolic deep reinforcement learning (2023) ICLR. For more details, find us at ICLR 2023!', 'RL problems can be described as a Markov Decision Process (MDP), where the agent observes some state s∈S from the environment’s state space, based on which it executes some action a∈A from its action space, and, finally, receives a reward r from its reward function r: S×A ↦ R.', 'The evolution of the environment relies on the markovian property, meaning that it is independent of past states given the current one, and is fully described by the transition dynamics P : S×A×S ↦ R and initial state distribution p₀: S ↦ R.', 'A policy is a parameterised distribution function over actions a∼π(⋅|s) given the current state s, representing the agent’s behaviour. Each episode of interaction between the agent and the environment produces a trajectory, τ = (s₀,a₀,s₁,a₁,…), according to the policy and transition dynamics. For each state s∈ S, the policy’s value function represents the expected discounted sum of future rewards over the agent’s possible trajectories starting from s [*].', 'The agent’s objective is to learn a policy maximising its expected discounted sum of rewards over encountered trajectories, or equivalently, the expected value function over the possible initial states. In deep RL, the policy and value functions are typically modeled as neural networks. The RL training loop involves alternating between an experience collection phase (deploying the current policy in the environment) and a learning phase (updating the agent’s models to improve its behaviour). Based on how the collected experience data is used, we can distinguish between two main classes of RL algorithms:', 'On-policy algorithms collect a new set of trajectories with the latest policy for each training iteration, discarding old data. They use these trajectories to learn the current policy’s value function which is then used to compute the policy gradient [1] and maximize the probability of performing the best-observed current actions. Proximal Policy Optimisation (PPO) [2] is currently one of the most established and robust algorithms within this class [3].', 'Off-policy algorithms instead store many different trajectories collected with a mixture of old policies in a large replay buffer dataset of experiences. They use this data to directly learn a model of the optimal value function using a squared loss based on the Bellman backup [4]. The policy is then implicitly defined based on the actions leading to the highest expected estimated value. Rainbow DQN [5] is a modern popular instantiation of the seminal off-policy DQN algorithm [6], introducing several auxiliary practices that stabilise and speed up learning.', 'Generalisation in Deep Reinforcement Learning', 'Generalisation is key requirement of effective RL agents, since most real-world and even complex simulated tasks entail a large degree of diversity in their state space (e.g., the space of natural images). From the agent’s perspective, exploring and memorising the exact value for this (possibly infinite) set of inputs is clearly intractable. Moreover, for many applications, controlled laboratory settings used for training might not reflect the full diversity of possible configurations for a given task. Therefore, the agent’s behaviour should ideally be robust to small distribution shifts it might observe during deployment.', 'Deep neural network-based agent models serve as a practical approach to tackle these problems, effectively serving as a functional prior that tries to capture only the most relevant and causal features of the states that the agent should require for effective decision-making. However, providing a precise understanding of how different design choices affect neural network training and its resulting generalisation is very much still an open question.', 'In our recent paper [7], we study the geometric properties that make a deep RL model generalise robustly and effectively. In particular, we focus on the model of hyperbolic geometry, which we describe next [8].', 'Most applications in machine learning (and more broadly, computer science and mathematics) utilise Euclidean spaces to represent data and perform numerical operations. Euclidean spaces can be easily visualised and most of their properties are inherently intuitive to understand. For instance, the total volume grows polynomially with the radius from the origin [9], and translating two points by the same vector does not affect their distance.', 'Hyperbolic spaces [10] do not possess such intuitive properties and, formally, can be described as a special type of Riemannian manifolds, i.e., n-dimensional objects embedded in n+1 dimensions that are only locally Euclidean. One of the defining properties of hyperbolic spaces is their constant negative curvature resulting in distances and volumes growing exponentially rather than polynomially.', 'This allows interpreting hyperbolic spaces as continuous analogs of trees, in which the number of leaf nodes also grows exponentially as we increase the depth. Due to this fact, a tree can be embedded isometrically (i.e., in a manner preserving the relative distances between nodes) in a hyperbolic space of only two dimensions [11]. In contrast, embedding a tree in a Euclidean space results in distortions, which can be decreased by using a high dimension.', 'There exist several equivalent models of hyperbolic geometry; here, we consider the Poincaré ball (denoted by 𝔹ⁿ), which can be conceptualised as an n-dimensional unit ball that preserves the notion of angles from Euclidean spaces. Since total volume of the Poincaré ball grows exponentially with the radius from the origin, the geodesics (shortest paths) are circular arcs perpendicular to the boundary rather than straight, as in Euclidean spaces [12].', 'In order to work with hyperbolic spaces in machine learning, we have to redefine standard operations with vectors, the notions of hyperplanes, and the relative distances between these elements [13]. The conceptual difficulty of doing this stems from the fact that we need to work in the tangent space, a local Euclidean representation of the hyperbolic space.', 'This is achieved by the exponential map expₓ(v), which takes a unit step along a geodesic starting from point x in the direction of an input vector v. We use the exponential map from the origin of the Poincaré ball to map Euclidean input vectors v into 𝔹ⁿ [14].', 'Gyrovector spaces [15] allow extending common vector operations to non-Euclidean geometries. One such operation is denoted by x⊕y and is referred to as Möbius addition [16].', 'Gyroplanes (denoted by H) are a generalisation of an oriented hyperplane in a gyrovector space. A gyroplane on the Poincaré ball is parameterised by n-dimensional shift p and normal w, such that H = {y∈ 𝔹ⁿ : <y⊕p,w>=0}.', 'In machine learning problems, hyperplanes and gyroplanes can be used as oriented decision boundaries. The shift and normal vectors (p and w) provide an alternative parametrisation to define linear affine transformation [17]. An analogy of a fully connected layer with m output units is a set of m gyroplanes in 𝔹ⁿ: given an n-dimensional input vector x in the hyperbolic space, the layer output f(x) is computed as the signed and scaled distance between x and each gyroplane H:', 'f(x) = 2 sign(<x⊕–p,w>) ||w||d(x,H) / (1 — ||p||²)¹ᐟ²,', 'where d(x,H) is the distance function between x and H on the Poincaré ball [18].', 'Similarly to prior work for supervised [19] and unsupervised learning [20], we use these parameterised gyroplane fully-connected layers within standard natural network architectures by substituting the standard Euclidean layer. The hyperbolic geometry of the resulting feature space introduces a different inductive bias that should be more appropriate for many RL problems, as we motivate next.', 'Due to the markovian property in RL problems, the evolution of the state in a trajectory can be conceptualised as a tree with the policy and dynamics determining the probability of ending up in each of its possible branches. Intuitively, the value and optimal policy for each state are naturally related to its possible successors in the MDP.', 'In contrast, there are many examples where other fixed, non-hierarchical information about the state (such as the general appearance of the environment), should be ignored. For instance, Raileanu and Fergus [21] observed the agents’ value function and policy models overfitting to spurious correlations from non-hierarchical features in Procgen environments [22] (e.g., background color), leading to poor generalisation to unseen levels.', 'Based on these observations, we hypothesise that effective features should encode information directly related to the hierarchical state relationships of the MDPs, reflecting its tree-like structure.', 'In order to validate our hypothesis, we analyze the representation spaces learned by RL agents, testing whether they manifest a hierarchical structure. We use the notion of Gromov δ-hyperbolicity [11]: a metric space (X,d) is said to be δ-hyperbolic if every possible geodesic triangle △ABC is δ-slim, i.e., for every point on any side of △ABC there exists some point on one of the other sides whose distance is at most δ. A characteristic of tree structures is that every point in any possible △ABC belongs to at least two of its sides, yielding δ=0. Thus, we can interpret δ-hyperbolicity as measuring the deviation of a given metric from a tree metric.', 'The final representations learned by an RL agent from encoding the collected states span some finite subset of Euclidean space, effectively yielding a discrete metric space. Similarly to [19], we normalise our δ-hyperbolicity recordings by the diameter of the space, yielding a relative measure that tries to be agnostic to the scale of the learned representations [23].', 'This allows us to practically interpret the hyperbolicity of the learned latent representations, spanning values between 0 (exact tree-like structure) and 1 (perfectly non-hyperbolic space). We train a standard PPO agent with the standard Impala architecture [24] and analyze how its performance and our δ-hyperbolicity measure evolve as training progresses, on four different Procgen environments.', 'We observe that δ quickly drops to low values (0.22–0.28) in the first training iterations in all environments, reflecting the largest relative improvements in agent performance. Subsequently, an interesting dichotomy seems to take place. In the fruitbot and starpilot environments, δ further decreases throughout training as the agent recovers high performance with a low generalisation gap between the training and test distribution of levels.', 'Instead, in bigfish and dodgeball, δ begins to increase again after the initial drop, suggesting that the latent representation space starts losing its hierarchical structure. Correspondingly, in these last two environments, the agent starts overfitting as test levels performance stagnates while the generalization gap with the training levels performance keeps increasing.', 'These results support our hypothesis, empirically showing the importance of encoding hierarchical features and suggesting that PPO’s poor generalisation in some environments is due to the observed tendency of Euclidean latent spaces to encode spurious features that hinder hyperbolicity.', 'Based on our findings, we propose to employ hyperbolic geometry to encode the final latent representations of deep RL models. Our approach has the objective of introducing a different inductive bias that incentivises modeling the agent’s policy and value function based on features that reflect the causal hierarchical evolution observed in common MDPs.', 'Our basic implementation tries to make minimal changes to the underlying algorithm and models. We start with a simple extension to PPO, replacing the final ReLU and linear layer with an exponential map to 𝔹ⁿ and a gyroplane fully-connected layer outputting the value function and policy logits.', 'However, this naïve approach results in underwhelming performance, lagging considerably behind the performance of standard PPO. Furthermore, we found that the hyperbolic policy struggles to start exploring and later annealing back to more deterministic behaviour as performance improves, as we would normally expect from PPO’s entropy bonus. These results appear to indicate the presence of optimisation challenges stemming from end-to-end RL training with hyperbolic representations.', 'To overcome similar optimisation issues, prior work using hyperbolic space for supervised and unsupervised learning introduced careful initialisation schemes [25] and stabilisation practices such as representation clipping [26]. While we do also make use of such practices in our implementation, they appear to be largely ineffective for RL problems.', 'This should not be surprising: the main purpose of these strategies is to facilitate learning appropriate angular layouts in the first few training iterations, without which later end-to-end optimisation can often result in failure modes with low performance [13]. However, the inherent high variance and non-stationarity characterising RL make stabilisation strategies focused mostly on early iterations insufficient. The trajectory data and loss landscapes in RL can change significantly throughout training, making early angular layouts inevitably suboptimal in the long term. Furthermore, the high variance policy gradient optimisation [1] can much more easily enter the aforementioned unstable learning regimes, leading to the observed failure modes.', 'Another sub-field of machine learning having to deal with similar non-stationarity and brittle optimisation is generative modeling with adversarial networks (GANs) [27]. In GAN training, the generated data and discriminator’s parameters constantly evolve, making the loss landscape highly non-stationary as in the RL setting. Furthermore, the adversarial nature of the optimisation makes it very brittle to exploding and vanishing gradients instabilities which lead to common failure modes [28].', 'We take inspiration from stabilisation practices in this parallel literature and make use of spectral normalisation (SN) [29], based on the recent analysis and empirical results showing its effects to precisely prevent exploding gradients phenomena [30]. Our implementation applies SN to all layers in the Euclidean encoder part of our model, leaving the final linear hyperbolic layer unregularised. Moreover, we also scale the final latent representations before mapping them to 𝔹ⁿ, such that modifying the dimensionality of the representations should not significantly affect their own and their gradients’ magnitudes. We call our stabilisation recipe Spectrally Regularised Hyperbolic Mappings (S-RYM, pronounced ɛs-raɪm).', 'Applying S-RYM to our hyperbolic RL agents appears to resolve their optimisation challenges. Furthermore, they also attain considerably higher performance relative to the original Euclidean implementation and maintain low gradient magnitudes throughout training.', 'We evaluate hyperbolic deep RL across different benchmarks, RL algorithms, and training conditions. In addition to PPO, we also apply our methodology to the off-policy Rainbow DQN algorithm. We test our agents on both the full Procgen benchmark (16 environments) [22] and the Atari 100K benchmark (26 environments) [31].', 'On the Procgen benchmark, we compare our hyperbolic implementations also to using random crop data augmentations [32], a more traditional way of incentivising generalisation by inducing hand-picked invariances. Moreover, we also test an alternative version of the hyperbolic model that further constrains the dimensionality of the final representation to 32 (from 256 in the original Euclidean architecture) with the aim of increasing its focus on features that can be efficiently encoded in hyperbolic space.', 'Both our hyperbolic PPO and Rainbow DQN implementations yield conspicuous performance gains in the great majority of the environments. Remarkably, we find that reducing the size of the hyperbolic representations provides even further benefits with significant improvements for both algorithms.', 'In contrast, applying data augmentations appears to yield lower and inconsistent gains. We also find that test performance gains do not always correlate with gains on the specific 200 training levels to which the agent has access for exploration, resulting in a significantly reduced generalisation gap for the hyperbolic agents.', 'Similarly, the exact same hyperbolic deep RL framework provides consistent and significant benefits also on the Atari100K benchmark. Hyperbolic Rainbow shows improvements over the Euclidean baseline on most Atari environments, almost doubling the final human normalised score.', 'Overall, our results empirically validate that introducing hyperbolic representations to shape the prior of deep RL models can be remarkably effective across a diverse range of problems and algorithms.', 'Our hyperbolic RL agents come very close to the current SotA algorithms which incorporate different expensive and domain-specific practices (e.g., ad-hoc auxiliary losses, larger specialised architecture, etc.). Taken together, we believe these results show the great potential of our hyperbolic framework to become a standard way of parameterising deep RL models.', 'Using a two-dimensional version of our hyperbolic PPO for visualisation purposes, we observe a recurring phenomenon, where the magnitude of the representations monotonically increases within the considered subsets of the trajectory as more obstacles and/or enemies appear in the environments. Furthermore, we observe that the representations form tree-like structures, with their magnitudes from encoding on-policy states growing in directions mostly aligned to the value function’s gyroplane’s normal.', 'This growth intuitively reflects that as new elements appear the agent recognises a larger opportunity for rewards (e.g., obtained for defeating its new enemies), yet, requiring also a finer level of control as distances to the other policy gyroplanes will also grow exponentially, reducing entropy. Instead, following random behaviour deviations, the representations’ magnitudes tend to grow in directions that appear almost orthogonal to the value gyroplane’s normal. Hence, this growth still reflects the higher precision required for optimal decision-making but also the agent’s higher uncertainty to obtain future rewards in states reached from suboptimal behaviour.', 'Our experiments provide strong evidence for the advantages and generality of using hyperbolic geometry in deep RL, yielding near-universal improvements across benchmarks and classes of RL algorithms. Our findings show that geometry can greatly affect the prior induced by learning with deep models, perhaps, suggesting we should re-evaluate its role and relevance to tackle many additional challenges in machine learning. For instance, using hyperbolic space could have implications also for unsupervised and offline RL, providing a more appropriate prior to deal with the under-specified objectives and limited data characterising these problem settings.', '[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (2018) MIT Press, provide a comprehensive introduction to the field of RL. See also other great online resources.', '[2] J. Schulman, Proximal policy optimization algorithms (2017) arXiv:1707.06347.', '[3] PPO improves stability by restricting the policy update from making changes >ϵ to its current probabilities and employs an auxiliary entropy bonus.', '[4] R. E. Bellman, Dynamic programming (2010) Princeton University Press.', '[5] M. Hessel et al., Rainbow: Combining improvements in deep reinforcement learning (2018) AAAI.', '[6] V. Mnih et al., Human-level control through deep reinforcement learning (2015) Nature 518 (7540):529–533.', '[7] E. Cetin et al., Hyperbolic deep reinforcement learning (2023) ICLR. See also the accompanying code.', '[8] For an overview of hyperbolic spaces and their early applications in machine learning, we refer to a great introductory blog post by Brian Keng. For a more formal introduction, see e.g. J. W. Cannon et al., Hyperbolic geometry (1997) in Flavors of Geometry 31:59–115.', '[9] Most know this from school geometry: the area of a circle (i.e., volume of a 2-dimensional ball) is πr². The general formula for the volume of an n-dimensional Euclidean ball of radius r is πⁿᐟ²rⁿ / Γ(n/2 +1). Notice that while it is polynomial in r, it is exponential in dimension n. For this reason, in order to represent tree-like structures (that have exponential volume growth) in a Euclidean space, one has to increase the dimension.', '[10] Hyperbolic geometry was the first successful construction of non-Euclidean geometry, in which the classical Parallels Postulate does not hold. Early unsuccessful attempts go back to Omar Khayyam and Giovanni Saccheri. The priority over the first successful construction is disputed between Carl Friedrich Gauss, János Bolyai, and Nikolai Lobachevsky (the first to publish his results). Eugenio Beltrami (and later Felix Klein) showed the self-consistency of hyperbolic geometry and proposed a projective model bearing their names (Beltrami-Klein).', '[11] M. Gromov, Hyperbolic groups (1987) Springer.', '[12] This makes geodesics between distinct points pass through some midpoint with a lower radius, analogously to how tree geodesics between nodes must go through their closest shared parent.', '[13] O. Ganea, G. Bécigneul, and T. Hofmann, Hyperbolic neural networks (2018) NeurIPS.', '[14] The exponential map is given in closed form as exp₀(v) = v tanh(v) / ||v||.', '[15] A. A. Ungar, Analytic hyperbolic geometry and Albert Einstein’s special theory of relativity (2008), World Scientific.', '[16] In hyperbolic space, the Möbius addition of two vectors is given by x⊕y = ((1 + 2x<x,y> + ||y||²)x + (1 + ||x||²)y) / (1 + 2<x,y> + ||x||² ||y||²), see equation (4) in our paper [7].', '[17] G. Lebanon and J. Lafferty, Hyperplane margin classifiers on the multinomial manifold (2004) ICML.', '[18] The distance is given in closed form as d(x,H)=sinhᐨ¹(2|<x⊕–p,w>| / (1 — ||x⊕–p||²||w||)), see equation (6) in our paper [7].', '[19] V. Khrulkov et al., Hyperbolic image embeddings (2020) CVPR.', '[20] E. Mathieu et al., Continuous hierarchical representations with Poincaré variational auto-encoders (2019) NeurIPS.', '[21] R. Raileanu and R. Fergus, Decoupling value and policy for generalization in reinforcement learning (2021), ICML.', '[22] Procgen, introduced by K. Cobbe et al., Leveraging procedural generation to benchmark reinforcement learning (2020) ICML, consists of 16 visual environments with procedurally-generated random levels. While the different levels share a high-level objective (e.g., reach the door, defeat all enemies, etc.), they might have considerable differences in their layout and appearance. Moreover, while training in this benchmark, the agents only have access to the first 200 levels of each environment for experience collection, but their performance is tested on the full distribution of levels. Hence, this benchmark allows assessing RL agents with a specific focus on their generalisation to unseen levels.', '[23] M. Borassi, A. Chessa, and G. Caldarelli, Hyperbolicity measures democracy in real-world networks (2015) Physical Review E 92.3: 032812.', '[24] L. Espeholt et al., Impala: Scalable distributed deep-RL with importance weighted actor-learner architectures (2018) ICML.', '[25] M. Nickel and D. Kiela, Poincaré embeddings for learning hierarchical representations (2017) NeurIPS.', '[26] Y. Guo et al., Clipped hyperbolic classifiers are super-hyperbolic classifiers (2022), CVPR.', '[27] I. Goodfellow et al., Generative adversarial nets (2014), NIPS.', '[28] M. Arjovsky and L. Bottou, Towards principled methods for training Generative Adversarial Networks (2017) ICLR.', '[29] T. Miyato et al., Spectral normalization for generative adversarial networks (2018) arXiv:1802.05957.', '[30] Z. Lin, V. Sekar, and G. Fanti, Why spectral normalization stabilizes GANs: Analysis and improvements (2021), NeurIPS.', '[31] Atari 100K, introduced by Kaiser, Lukasz, et al., Model-based reinforcement learning for Atari (2019) arXiv:1903.00374, consists of 26 different visual environments consisting of iconic Atari games from M. G. Bellemare et al., The arcade learning environment: An evaluation platform for general agents (2013) Journal of Artificial Intelligence Research 47: 253–279. However, agents are allowed access only to 100K total environment steps of data for experience collection, corresponding roughly to 2 hours of playtime. The environments are modified with the specifications from M. C. Machado et al., Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents (2018) Journal of Artificial Intelligence Research 61:523–562, introducing considerable randomness through sticky actions (i.e., random repeats of each executed action). Thus, due to the severe data constraints and added randomness, this benchmark places a specific focus on assessing generalization to unseen states.', '[32] D. Yarats, I. Kostrikov, and R. Fergus, Image augmentation is all you need: Regularizing deep reinforcement learning from pixels (2021), ICLR.', 'We are grateful to David Ha (a.k.a. hardmaru) for generating the title image, the first AI-generated illustration on this blog! See additional information on the project webpage, Towards Data Science Medium posts, subscribe to Michael’s posts and YouTube channel, get Medium membership, or follow Michael, Edoardo, Ben, and Jonathan on Twitter.']
04/30/2023	Data Analysis Made Easy: Using LLMs to Automate Tedious Tasks	-	179	0	https://towardsdatascience.com/@jyesr	https://towardsdatascience.com/data-analysis-made-easy-using-llms-to-automate-tedious-tasks-bdc1fee552d5?source=collection_archive---------2-----------------------	11	6	['Data Analysis Made Easy: Using LLMs to Automate Tedious Tasks', 'How LLMs can help', 'Examples of using LLMs', 'Which LLM to use?', 'Tips for using LLMs effectively', 'Conclusion']	78	"['Member-only story', 'Jye Sawtell-Rickson', 'Follow', 'Towards Data Science', '--', 'Listen', 'Share', 'Data analysis can be equal parts challenging and rewarding. From cleaning messy datasets to building complex models, there’s always a lot to do, and not enough time to do it. But what if there was a way to streamline and automate some of the more routine tasks, freeing up more time for strategic thinking and decision-making? That’s where LLMs come in.', 'Large Language Models (LLMs) are AI language models that can assist with a wide range of natural language processing tasks, from generating text to answering questions. And as it turns out, they can also be a valuable tool for data analysts. In this article, we’ll explore some of the ways you can use LLMs in your day-to-day work as a data analyst, and show you how AI can help you work smarter, not harder.', 'Let’s jump straight into it.', 'Note: these systems are not (yet) an end-to-end analyst solution that will replace you. Stay tuned to the space though.', 'LLMs can act as AI-powered chatbots that can assist with streamlining and automating tasks related to data analysis. With their advanced capabilities, LLMs can help with a variety of tasks. I’ve classified them into three broad categories:', 'Putting all these into practice can save a significant amount of time and effort throughout your time as a data analyst.', 'Let’s explore some examples of these to see just how powerful and flexible the tools are today.', 'This section will contain examples of the application of LLMs. The examples are mostly indicated with a different format to highlight the responses, otherwise you might confuse it with my writing!', 'Throughout this article as we prompt the LLM, we’ll use an excerpt from the Spotify and Youtube songs dataset that includes the column information and first 20 rows of the dataset. In the future, an LLM could have direct access to the dataset, as well as well as responses to code it executes on the dataset, to remove the limitations of such a small sample.', 'LLMs trained on codebases are competent coders as seen in this article. This means that they can readily solve common technical data analyst tasks. They can perform very well with tasks that require little context and technical tasks more commonly fall into this bucket.', 'One of the most basic tasks of a data analyst is performing an exploratory data analysis (EDA). LLMs are able to write Python code that can explore a dataset as well as output the corresponding images. The following quoted text contains responses returned by an LLM when prompted to do an EDA:', 'These are fairly basic, but LLMs can be pushed further to provide other EDA approaches. In this case, they’re able to offer up outlier detection and clustering as further ways to explore the data, among others.', 'An example of the response given is below. Reading through this, it could definitely pass as work by a junior analyst, identifying some insights such as:', '“The danceability values for the Gorillaz songs are generally higher than that of Red Hot Chili Peppers.”', '“The data is about songs from two artists, Gorillaz and Red Hot Chili Peppers. Gorillaz has 9 songs in the given data while Red Hot Chili Peppers has 4 songs.', 'The danceability and energy values for the songs vary widely between 0.559 to 0.938 and 0.345 to 0.938, respectively. The danceability values for the Gorillaz songs are generally higher than that of Red Hot Chili Peppers. The energy values for Gorillaz songs also tend to be higher than that of Red Hot Chili Peppers.', 'The keys for the songs range from 1.0 to 11.0, with no discernible pattern or trend.”', '(full output)', 'When creating tables, it can be a little annoying to write good descriptions for columns and the overall table. While this is good practice, it is often overlooked. LLMs can do this by guessing the context of the table, or even better, can write them when some context is provided.', '“This table appears to show some information about various songs by two different artists — Gorillaz and Red Hot Chili Peppers. Each row in the table represents a different song, and there are 10 columns of information provided for each song. Here are the descriptions of the columns:', 'Index: This is just an integer index, which appears to be a unique identifier for each song in the table.', 'Artist: This column lists the artist who created the song.', 'Link: This column provides a link to the artist’s page on Spotify.', 'Song Name: This column lists the name of the song.', 'Album Name: This column lists the name of the album that the song appears on.', 'Album: This column indicates whether the row represents an album or a single.', 'Source Identifier: This column appears to be a unique identifier for each song, possibly specific to Spotify.', 'Danceability: This column provides a score from 0 to 1 indicating how suitable the song is for dancing.', 'Energy: This column provides a score from 0 to 1 indicating the energy level of the song.', 'Key: This column provides a number from 0 to 11 indicating the musical key of the song.', 'Overall, this table appears to be a collection of song data, including information about the artist, album, and various musical features of each song.”', 'Some impressive parts of these results is that the LLM has looked through the data samples to work out that the “Key” column is not some key for the rows, but the musical key for the song, and the “Album” column describes whether it is an album or single. A typical analyst wouldn’t be able to guess these just looking at the column names and would have to waste time doing the basic EDA to work out what value the data source can provide — LLMs save this time.', 'The main use-case today for augmented intelligence in data analysis is in technical question answering by the likes of Stack Overflow. However LLMs are often superior, providing the correct code for your specific use-case and variables, as well as the ability to give deeper answers or tutorials as a follow-up.', 'For example, plotting a Wordcloud is something that is required every now and then, but it’s difficult to remember the exact library and code to use to plot one. You can simply ask an LLM, and it will happily return you working code. The key excerpt from that below:', '# Generate the wordcloud', ""wordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = set(), min_font_size = 10).generate(wordcloud_text)"", 'I realised that this is using just a list of words but I had word counts, so I quickly followed up “Can I feed in weighted words instead?” and the LLM re-did the code:', '# Generate the wordcloud', ""wordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = set(), min_font_size = 10).generate_from_frequencies(weighted_words)"", '(full output)', 'This wouldn’t have been possible with Stack Overflow, and saved plenty of time Googling or looking through old code.', 'As can be seen, LLMs are very capable at technical questions and can save time in the data analysis workflow, but what else are they capable of?', 'Soft-skills may be overlooked as a requirement for data analysis, but they’re crucial to ensure any analysis is useful for the business.', 'Any data analyst knows that understanding what data the people you work with actually want is one of the toughest parts. This can go wrong in many ways, from the partner making a simple mistake in what something is called, to extending what they want as soon as you return the first part, to asking for the wrong data that doesn’t support the conclusion they want to make. Due to their conversational nature, LLMs are great at collecting requirements from partners. An example conversation is below.', '“I need you to pull the latest animated sticker numbers.', 'Great, thanks for your request! Just to clarify, when you say “animated sticker numbers”, can you please provide more information on what exactly you mean by that?', 'Are you looking for data on the number of animated stickers that have been created? Or are you interested in the usage metrics of animated stickers, such as the number of times they’ve been viewed or downloaded?', 'Additionally, could you please specify the time frame you’re interested in? Are you looking for data on the latest animated sticker numbers for a specific day, week, or month? Finally, would you like me to segment the data in any way, such as by sticker category or by user demographic?”', 'Practically, it may be difficult to ask your partners to go chat to an LLM first, but LLMs are useful for figuring out if there are things you should have clarified further before kicking off work on a project, and in the future will be integrated into analytics solutions directly.', 'Once the requirements are collected and the analysis is done, the findings must be communicated back. LLMs already have a track record of helping writers to produce content faster, and of a higher quality. In the same way, an LLM can help with preparing documents summarising analysis work, or prepping presentations. Some particular tasks that LLMs are useful at are basic introductions and conclusions, as well as text summarisation.', '“Based on this EDA, the song recommendation service might consider adding more Gorillaz songs to their recommendation algorithm and prioritize songs with higher danceability and energy values, as these seem to be popular among their users.”', '(full output)', 'Some tips for using LLMs for writing can be found in this post, but there are many more out there and more will be added as we continue how best to work with LLMs.', 'We’ve now seen how LLMs can solve technical tasks and assist with a lot of the ‘soft’ tasks. This is already great for improving an analyst’s workflow, but can we take this further?', 'Strategic thinking is one of the highest value an analyst can offer to a company. This can be enhanced with LLMs. Some the ways this can be done include brainstorming what analyses to run, creating broad understanding frameworks, improving and iterating on your analytical approach and as a general thought-partner.', 'Let’s start with brainstorming. You may have been tasked with the generic task of “understanding more about your customers”. What sort of analyses could you do about this? What outcomes would they lead to? Ideally you would brainstorm with your peers, but that’s not always possible, so you can brainstorm with an AI instead.', 'When asked what analyses we should run to understand our customers better, the outputs included: “Customer segmentation, RFM analysis, Basket analysis, Churn analysis, Customer lifetime value.” With some predicted outcomes:', '“The outcomes of these analyses will depend on the specific insights you uncover. For example, you might discover that your most valuable customers are older, wealthy women who are interested in specialty cooking items. This could lead you to develop more products targeted towards this demographic and to invest in marketing campaigns that resonate with them.”', '(full output)', 'That’s a great starting point, and can make sure you’re covering all the gaps, which can be difficult when working alone. LLMs can also give advice on frameworks. You might be wondering how you can grow your business over the next few years, and how you might measure that, well:', '“There are several growth frameworks that can be used to understand how to grow sales over the next three years. Here are three popular frameworks:', 'AARRR: This framework stands for Acquisition, Activation, Retention, Referral, and Revenue. It is a five-step framework developed by Dave McClure that is commonly used in startup circles. The AARRR framework helps companies understand the different stages of the customer lifecycle and how to optimize each stage to drive growth.', 'RACE: This framework stands for Reach, Act, Convert, Engage. It is a four-stage framework developed by Smart Insights that is focused on digital marketing. The RACE framework helps companies understand how to optimize their digital marketing efforts to drive growth.', '3HAG: This framework stands for Three-Year Highly Achievable Goal. It is a strategic planning framework developed by Shannon Susko that helps companies set and achieve ambitious growth goals. The 3HAG framework involves defining a BHAG (Big Hairy Audacious Goal) and then breaking it down into smaller, achievable goals over the course of three years.', 'When deciding which growth framework to use, it’s important to consider your company’s unique situation and goals. AARRR and RACE are more tactical frameworks that focus on optimizing specific stages of the customer lifecycle or digital marketing efforts, while 3HAG is a more strategic framework focused on setting ambitious growth goals.”', 'As you can see, the response is helpful and could certainly stand in for a response you’d expect from an analyst in your team.', 'As of writing this, there’s really only one LLM that can do all the above tasks well, and that’s OpenAI’s ChatGPT. It was the first to popularise LLMs and remains one of the strongest offerings on the market, and is free (with a subscription option).', 'With the pace of change we’re seeing in AI, it’s very possible this won’t be true in a few months, so it’s worth noting that there are plenty of other competitors. For example, Google is developing their product, Bard, which is expected to perform similarly to ChatGPT. There are also many open source alternatives to consider. While these are generally not of the same quality, they’re expected to keep improving and close the gap between commercially operated models.', 'To get the most out of LLMs as a data analyst, there are a few tips you can follow. First, it’s important to give clear and specific inputs to LLMs. This means using proper language, avoiding ambiguity, and providing context where necessary. Additionally, LLMs can work with both structured and unstructured data, so it’s worth experimenting with different input formats to see which works best for a given task. Finally, it’s important to remember that LLMs are a tool, not a replacement for human analysis. While it can help automate some routine tasks, it’s still up to the data analyst to interpret the results and make informed decisions based on the data.', 'There are plenty of articles out there such as this one discussing how to work with LLMs and it’s a growing field of study, so keep learning!', 'In conclusion, LLMs are a great tool to improve the efficiency of your analytics work and even to grow and learn new things. LLMs can help with technical problems, develop soft skills and improve your strategic thinking. Working with AI is the future, so now is the best time to start learning how to integrate it into your workflow so you’re not left behind.']"
04/30/2023	What is dbt (data build tool) and When should you use it?	Discover the Hidden Benefits and Drawbacks…	380	0	https://towardsdatascience.com/@khuyentran1476	https://towardsdatascience.com/what-is-dbt-data-build-tool-and-when-should-you-use-it-9dc1566d960d?source=collection_archive---------3-----------------------	8	12	['What is dbt (data build tool) and When should you use it?', 'Motivation', 'What is dbt?', 'When Should You Consider dbt', 'Setup dbt', 'Modularity', 'Code Reusability', 'Testing', 'Documentation', 'Track changes in data', 'When You Should Not Use dbt', 'Conclusion']	55	['Member-only story', 'Khuyen Tran', 'Follow', 'Towards Data Science', '--', 'Listen', 'Share', 'If your organization is looking to create a data-driven product, you should consider having efficient data pipelines to:', 'One tool that has gained popularity in recent years for managing data pipelines is dbt (data build tool).', 'While dbt can provide significant benefits for managing and modeling data, it may not be the best tool for every situation. In this article, we’ll explore the use cases of dbt to determine whether dbt is a good fit for your organization.', 'Feel free to play and fork the source code of this article here:', 'github.com', 'dbt is an open-source tool for transforming data in a data warehouse. With dbt, users can write SQL queries that transform data and create repeatable workflows that can be easily tested and automated.', 'You should consider using dbt when:', 'Before diving into some helpful features of dbt, we need to first set up the dbt environment. This will allow you to easily follow along with the discussion.', 'This article uses BigQuery as a data warehouse for dbt, to install dbt’s adapter for BigQuery, type:', 'Follow this instruction if you want to use other dbt’s adapters.', 'To initialize a dbt project called dbt_demo, type the following:', 'The following is the project directory.', 'dbt’s modular approach allows you to break down a complex pipeline into smaller pieces.', 'To demonstrate this, we will use the austin_bikeshare dataset from BigQuery public datasets.', 'Create a new SQL file in the models directory, named models/trips_per_property.sql with the following code:', 'Since this model contains multiple data transformations, it can be difficult to maintain and test.', 'We could separate the model into different models by doing the following:', 'By breaking down the model into multiple models, you can now reuse the data models in other models:', 'Your team members can also work on the different parts of the project simultaneously.', 'Because each module is self-contained, it can be easily tested and validated.', 'Using macros in dbt simplifies the process of reusing SQL code fragments across various models.', 'To demonstrate, we will create a macro called get_end_time in the macros/get_end_time.sql file. This micro gets an end time based on a start time and duration.', 'Think of the get_end_time macro as a Python function that can be used repeatedly to generate a value based on the inputs provided.', 'Another model can now use this micro with specific values for start_time and duration_minutes:', 'With dbt, you can easily verify the correctness of your models. For instance, to ensure the stg_stations model is correct, you may want to check that:', 'To test the stg_stations model, create a new YAML file in the models directory, named models/schema.yml with the following content.', 'Then run dbt test to run all of the tests.', 'By catching potential errors early in the transformation process, tests can reduce the risk of errors in downstream applications and analyses.', 'In addition, tests provide a way to track changes in the data over time and to ensure that the transformation logic remains correct as the underlying data changes.', 'With dbt, you can easily document your models and share the documentation with your team.', 'To create documentation for your models, follow these steps:', 'View full file.', 'Documenting your data models helps to ensure that everyone who works with the models can comprehend their functionality and purpose.', 'You can also examine data lineage in the documentation by clicking “View lineage graph.”', 'With dbt, you can track data changes over time through snapshots.', 'To see why this is useful, imagine you have a subscriber_type table where the subscriber_type field is frequently overwritten as subscribers modify their membership status.', 'If a user changes their membership from “trial” to “premium”, the record of when the user had the “trial” membership will be lost.', 'Losing this record can be problematic if we want to use it for other evaluations such as calculating the average time taken by a user to change their subscription.', 'dbt can snapshot these changes to help you understand how values in a row change over time. Here’s an example of a snapshot table for the previous example:', 'View this tutorial on how to create snapshots.', 'While dbt can provide significant benefits for managing and modeling data, you should not use dbt when:', 'In this article, we cover some helpful features of dbt and learn when to and not to use dbt. I hope this article gives you the knowledge needed to decide whether dbt is the right solution for your data pipelines.', 'I love writing about data science concepts and playing with different data science tools. You can connect with me on LinkedIn and Twitter.', 'Star this repo if you want to check the code for the articles I have written. Follow me on Medium to stay notified about my latest data science articles:', 'towardsdatascience.com', 'towardsdatascience.com', 'towardsdatascience.com', 'towardsdatascience.com']
